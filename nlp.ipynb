{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db14f4a4",
   "metadata": {},
   "source": [
    "# Thuto Wesley Sephai\n",
    "## Umuzi Experience Gig XPL2\n",
    "## Check-in Feedback Analysis - 11 November 2025\n",
    "\n",
    "\n",
    "### Project Description\n",
    "NLP Trend Identification\n",
    "\n",
    "Conduct a thorough Natural Language Processing (NLP) analysis of the student check-in data. Primary goal is to transform the unstructured text in the \"Win\", \"Loss\", and \"Blocker\" columns into quantifiable, actionable insights. By identifying and quantifying the most frequent topics and sentiment trends uncover the core experiences of the student cohort. This analysis will directly inform program management about what is working well and where immediate improvements are needed.\n",
    "\n",
    "\n",
    "### What are the expected outcomes?\n",
    "\n",
    "1. Code Repository: A dedicated Git repository containing a self-contained Jupyter Notebook or Python script\n",
    "\n",
    "2. Top Trends Summary: A concise report, summarizing the:\n",
    "- Top 5 most frequent Wins themes\n",
    "- Top 5 most frequent Losses themes\n",
    "- Top 5 most frequent Blockers themes\n",
    "\n",
    "3. Visual Dashboard: Data visualisations that illustrate the frequency and distribution of the identified themes.\n",
    "\n",
    "4. Recommendations: A concluding section that provides 3-5 concrete recommendations for the program team based on your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba96f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526373d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('Copy of Umuzi XB1 Check in (Responses) - Form Responses 1 - Copy of Umuzi XB1 Check in (Responses) - Form Responses 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbea8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Timestamp  Column 2  Full name Please enter the date today  \\\n",
      "0  7/9/2025 14:34:49       NaN  Student 1                    7/9/2025   \n",
      "1  7/9/2025 14:43:15       NaN  Student 2                    7/9/2025   \n",
      "2  7/9/2025 14:49:40       NaN  Student 3                    7/9/2025   \n",
      "3  7/9/2025 14:50:41       NaN  Student 4                    7/9/2025   \n",
      "4  7/9/2025 15:14:46       NaN  Student 5                    7/9/2025   \n",
      "\n",
      "  Share a win from the last week (what went well, something you enjoyed)  \\\n",
      "0  Completing my first week with Umuzi gave me co...                       \n",
      "1  I enjoyed introspecting myself on the basis of...                       \n",
      "2  Submitting all my work in time and completing ...                       \n",
      "3       I submitted most of the assigned assignments                       \n",
      "4  I enjoyed the Life Lifeline activity. I got to...                       \n",
      "\n",
      "  Share a loss (something that was challenging or did not go well)  \\\n",
      "0  I didn’t get opportunities from two companies ...                 \n",
      "1  Except for being sick and experiencing challen...                 \n",
      "2                                   I don’t have any                 \n",
      "3  I did not understand some assignments s well a...                 \n",
      "4                                                NaN                 \n",
      "\n",
      "  Share a blocker, if any (anything that stopped you from doing what you needed to do)  \\\n",
      "0  Being financially unstable has been draining m...                                     \n",
      "1  None, only temporary set backs (reception and ...                                     \n",
      "2  Data , I couldn’t join some meetings because I...                                     \n",
      "3  Spending most time in class leading to having ...                                     \n",
      "4  I forgot to login to Google classroom, until I...                                     \n",
      "\n",
      "        Anything else you would like to share or ask  \n",
      "0  I appreciate Umuzi for this opportunity to sho...  \n",
      "1                                   Nothing for now.  \n",
      "2                                       No thank you  \n",
      "3  In overall, I am doing well and trying to do a...  \n",
      "4                                                No.  \n"
     ]
    }
   ],
   "source": [
    "# print the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eaabce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Column 2', 'Full name', 'Please enter the date today',\n",
       "       'Share a win from the last week (what went well, something you enjoyed)',\n",
       "       'Share a loss (something that was challenging or did not go well)',\n",
       "       'Share a blocker, if any (anything that stopped you from doing what you needed to do)',\n",
       "       'Anything else you would like to share or ask'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de73bd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8f6b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp                                                                                 0\n",
       "Column 2                                                                                372\n",
       "Full name                                                                                 0\n",
       "Please enter the date today                                                               0\n",
       "Share a win from the last week (what went well, something you enjoyed)                   12\n",
       "Share a loss (something that was challenging or did not go well)                         69\n",
       "Share a blocker, if any (anything that stopped you from doing what you needed to do)     95\n",
       "Anything else you would like to share or ask                                            154\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a81444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on relevant columns\n",
    "Win = \"Share a win from the last week (what went well, something you enjoyed)\"\n",
    "Loss = \"Share a loss (something that was challenging or did not go well)\"\n",
    "Blocker = \"Share a blocker, if any (anything that stopped you from doing what you needed to do)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e597af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values\n",
    "df[Win].fillna(\"\", inplace=True)\n",
    "df[Loss].fillna(\"\", inplace=True)\n",
    "df[Blocker].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65363c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Geeks2_PC12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Geeks2_PC12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Geeks2_PC12\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# stopwords and lemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d50757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18735fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle spelling variations\n",
    "def correct_spelling(text):\n",
    "    # This is a placeholder for a spelling correction function.\n",
    "    # In practice, you might use libraries like TextBlob or SymSpell.\n",
    "    return text\n",
    "# contraction handling\n",
    "contraction_mapping = { \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\" }\n",
    "\n",
    "# incosistency phrases handling\n",
    "def expand_contractions(text):\n",
    "    for contraction, full_form in contraction_mapping.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb64a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "# Converting text into numerical features using techniques ising TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "def vectorize_text(corpus):\n",
    "    X = tfidf_vectorizer.fit_transform(corpus)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "df[Win] = df[Win].apply(expand_contractions).apply(correct_spelling).apply(preprocess_text)\n",
    "X_win = vectorize_text(df[Win])\n",
    "df[Loss] = df[Loss].apply(expand_contractions).apply(correct_spelling).apply(preprocess_text)\n",
    "X_loss = vectorize_text(df[Loss])\n",
    "df[Blocker] = df[Blocker].apply(expand_contractions).apply(correct_spelling).apply(preprocess_text)\n",
    "X_blocker = vectorize_text(df[Blocker])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of the resulting matrices\n",
    "print(X_win.shape, X_loss.shape, X_blocker.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

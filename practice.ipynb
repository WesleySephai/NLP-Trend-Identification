{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab979204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. PROJECT OVERVIEW\n",
    "# -----------------------------------------------------------\n",
    "# This notebook performs NLP analysis on student feedback data.\n",
    "# Goals:\n",
    "# - Extract top themes from Wins, Losses, and Blockers\n",
    "# - Perform sentiment analysis\n",
    "# - Visualize trends\n",
    "# - Provide actionable recommendations\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. IMPORT LIBRARIES\n",
    "# -----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. LOAD AND INSPECT DATA\n",
    "# -----------------------------------------------------------\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Copy of Umuzi XB1 Check in (Responses) - Form Responses 1.csv\")\n",
    "\n",
    "# Check columns\n",
    "print(df.columns)\n",
    "\n",
    "# Focus on relevant columns\n",
    "win_col = \"Share a win from the last week (what went well, something you enjoyed)\"\n",
    "loss_col = \"Share a loss (something that was challenging or did not go well)\"\n",
    "blocker_col = \"Share a blocker, if any (anything that stopped you from doing what you needed to do)\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. DATA CLEANING & PREPROCESSING\n",
    "# -----------------------------------------------------------\n",
    "# Handle missing values\n",
    "df[win_col].fillna(\"\", inplace=True)\n",
    "df[loss_col].fillna(\"\", inplace=True)\n",
    "df[blocker_col].fillna(\"\", inplace=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. TOKENIZATION, STOPWORD REMOVAL, LEMMATIZATION\n",
    "# -----------------------------------------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "wins_tokens = [token for sentence in df[win_col] for token in preprocess(sentence)]\n",
    "losses_tokens = [token for sentence in df[loss_col] for token in preprocess(sentence)]\n",
    "blockers_tokens = [token for sentence in df[blocker_col] for token in preprocess(sentence)]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. FREQUENCY ANALYSIS (TOP THEMES)\n",
    "# -----------------------------------------------------------\n",
    "wins_top5 = Counter(wins_tokens).most_common(5)\n",
    "losses_top5 = Counter(losses_tokens).most_common(5)\n",
    "blockers_top5 = Counter(blockers_tokens).most_common(5)\n",
    "\n",
    "print(\"Top 5 Wins Themes:\", wins_top5)\n",
    "print(\"Top 5 Losses Themes:\", losses_top5)\n",
    "print(\"Top 5 Blockers Themes:\", blockers_top5)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7. SENTIMENT ANALYSIS\n",
    "# -----------------------------------------------------------\n",
    "wins_sentiment = [TextBlob(str(text)).sentiment.polarity for text in df[win_col]]\n",
    "losses_sentiment = [TextBlob(str(text)).sentiment.polarity for text in df[loss_col]]\n",
    "\n",
    "wins_summary = {\n",
    "    'positive': sum(1 for s in wins_sentiment if s > 0),\n",
    "    'neutral': sum(1 for s in wins_sentiment if s == 0),\n",
    "    'negative': sum(1 for s in wins_sentiment if s < 0)\n",
    "}\n",
    "\n",
    "losses_summary = {\n",
    "    'positive': sum(1 for s in losses_sentiment if s > 0),\n",
    "    'neutral': sum(1 for s in losses_sentiment if s == 0),\n",
    "    'negative': sum(1 for s in losses_sentiment if s < 0)\n",
    "}\n",
    "\n",
    "print(\"Wins Sentiment Summary:\", wins_summary)\n",
    "print(\"Losses Sentiment Summary:\", losses_summary)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8. VISUALIZATIONS\n",
    "# -----------------------------------------------------------\n",
    "# Bar charts for top themes\n",
    "sns.barplot(x=[w[0] for w in wins_top5], y=[w[1] for w in wins_top5])\n",
    "plt.title(\"Top 5 Wins Themes\")\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=[l[0] for l in losses_top5], y=[l[1] for l in losses_top5])\n",
    "plt.title(\"Top 5 Losses Themes\")\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=[b[0] for b in blockers_top5], y=[b[1] for b in blockers_top5])\n",
    "plt.title(\"Top 5 Blockers Themes\")\n",
    "plt.show()\n",
    "\n",
    "# Word Clouds\n",
    "for name, tokens in [('Wins', wins_tokens), ('Losses', losses_tokens), ('Blockers', blockers_tokens)]:\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tokens))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{name} Word Cloud\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 9. INSIGHTS & RECOMMENDATIONS\n",
    "# -----------------------------------------------------------\n",
    "print(\\\"\\\\nRecommendations:\\\")\n",
    "print(\\\"1. Improve internet/data support for students (frequent blocker).\\\")\n",
    "print(\\\"2. Offer time management workshops (common loss theme).\\\")\n",
    "print(\\\"3. Provide financial assistance or guidance (blocker and loss theme).\\\")\n",
    "print(\\\"4. Enhance clarity in instructions and resources (loss theme).\\\")\n",
    "print(\\\"5. Continue motivational and career planning activities (win theme).\\\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 10. EXPORT RESULTS\n",
    "# -----------------------------------------------------------\n",
    "# Save summary as JSON\n",
    "import json\n",
    "summary = {\n",
    "    'wins_top5': wins_top5,\n",
    "    'losses_top5': losses_top5,\n",
    "    'blockers_top5': blockers_top5,\n",
    "    'wins_sentiment': wins_summary,\n",
    "    'losses_sentiment': losses_summary\n",
    "}\n",
    "with open('analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab979204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\thuto\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\thuto\\anaconda3\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from wordcloud) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\thuto\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\thuto\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. PROJECT OVERVIEW\n",
    "# -----------------------------------------------------------\n",
    "# This notebook performs NLP analysis on student feedback data.\n",
    "# Goals:\n",
    "# - Extract top themes from Wins, Losses, and Blockers\n",
    "# - Perform sentiment analysis\n",
    "# - Visualize trends\n",
    "# - Provide actionable recommendations\n",
    "# -----------------------------------------------------------\n",
    "#installing required packages\n",
    "# install nltk if not already installed\n",
    "!pip install nltk\n",
    "#install wordcloud if not already installed\n",
    "!pip install wordcloud\n",
    "# install textblob if not already installed\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "844d0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. IMPORT LIBRARIES\n",
    "# -----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39464699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Thuto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thuto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Thuto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e8470ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. LOAD AND INSPECT DATA\n",
    "# -----------------------------------------------------------\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Copy of Umuzi XB1 Check in (Responses) - Form Responses 1 - Copy of Umuzi XB1 Check in (Responses) - Form Responses 1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f4ea540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Column 2</th>\n",
       "      <th>Full name</th>\n",
       "      <th>Please enter the date today</th>\n",
       "      <th>Share a win from the last week (what went well, something you enjoyed)</th>\n",
       "      <th>Share a loss (something that was challenging or did not go well)</th>\n",
       "      <th>Share a blocker, if any (anything that stopped you from doing what you needed to do)</th>\n",
       "      <th>Anything else you would like to share or ask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/9/2025 14:34:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student 1</td>\n",
       "      <td>7/9/2025</td>\n",
       "      <td>Completing my first week with Umuzi gave me co...</td>\n",
       "      <td>I didn’t get opportunities from two companies ...</td>\n",
       "      <td>Being financially unstable has been draining m...</td>\n",
       "      <td>I appreciate Umuzi for this opportunity to sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/9/2025 14:43:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student 2</td>\n",
       "      <td>7/9/2025</td>\n",
       "      <td>I enjoyed introspecting myself on the basis of...</td>\n",
       "      <td>Except for being sick and experiencing challen...</td>\n",
       "      <td>None, only temporary set backs (reception and ...</td>\n",
       "      <td>Nothing for now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/9/2025 14:49:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student 3</td>\n",
       "      <td>7/9/2025</td>\n",
       "      <td>Submitting all my work in time and completing ...</td>\n",
       "      <td>I don’t have any</td>\n",
       "      <td>Data , I couldn’t join some meetings because I...</td>\n",
       "      <td>No thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/9/2025 14:50:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student 4</td>\n",
       "      <td>7/9/2025</td>\n",
       "      <td>I submitted most of the assigned assignments</td>\n",
       "      <td>I did not understand some assignments s well a...</td>\n",
       "      <td>Spending most time in class leading to having ...</td>\n",
       "      <td>In overall, I am doing well and trying to do a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/9/2025 15:14:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Student 5</td>\n",
       "      <td>7/9/2025</td>\n",
       "      <td>I enjoyed the Life Lifeline activity. I got to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I forgot to login to Google classroom, until I...</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Timestamp  Column 2  Full name Please enter the date today  \\\n",
       "0  7/9/2025 14:34:49       NaN  Student 1                    7/9/2025   \n",
       "1  7/9/2025 14:43:15       NaN  Student 2                    7/9/2025   \n",
       "2  7/9/2025 14:49:40       NaN  Student 3                    7/9/2025   \n",
       "3  7/9/2025 14:50:41       NaN  Student 4                    7/9/2025   \n",
       "4  7/9/2025 15:14:46       NaN  Student 5                    7/9/2025   \n",
       "\n",
       "  Share a win from the last week (what went well, something you enjoyed)  \\\n",
       "0  Completing my first week with Umuzi gave me co...                       \n",
       "1  I enjoyed introspecting myself on the basis of...                       \n",
       "2  Submitting all my work in time and completing ...                       \n",
       "3       I submitted most of the assigned assignments                       \n",
       "4  I enjoyed the Life Lifeline activity. I got to...                       \n",
       "\n",
       "  Share a loss (something that was challenging or did not go well)  \\\n",
       "0  I didn’t get opportunities from two companies ...                 \n",
       "1  Except for being sick and experiencing challen...                 \n",
       "2                                   I don’t have any                 \n",
       "3  I did not understand some assignments s well a...                 \n",
       "4                                                NaN                 \n",
       "\n",
       "  Share a blocker, if any (anything that stopped you from doing what you needed to do)  \\\n",
       "0  Being financially unstable has been draining m...                                     \n",
       "1  None, only temporary set backs (reception and ...                                     \n",
       "2  Data , I couldn’t join some meetings because I...                                     \n",
       "3  Spending most time in class leading to having ...                                     \n",
       "4  I forgot to login to Google classroom, until I...                                     \n",
       "\n",
       "        Anything else you would like to share or ask  \n",
       "0  I appreciate Umuzi for this opportunity to sho...  \n",
       "1                                   Nothing for now.  \n",
       "2                                       No thank you  \n",
       "3  In overall, I am doing well and trying to do a...  \n",
       "4                                                No.  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1fb6f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Timestamp', 'Column 2', 'Full name', 'Please enter the date today',\n",
      "       'Share a win from the last week (what went well, something you enjoyed)',\n",
      "       'Share a loss (something that was challenging or did not go well)',\n",
      "       'Share a blocker, if any (anything that stopped you from doing what you needed to do)',\n",
      "       'Anything else you would like to share or ask'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cd9518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Share a win from the last week (what went well, something you enjoyed)  \\\n",
      "0  Completing my first week with Umuzi gave me co...                       \n",
      "1  I enjoyed introspecting myself on the basis of...                       \n",
      "2  Submitting all my work in time and completing ...                       \n",
      "3       I submitted most of the assigned assignments                       \n",
      "4  I enjoyed the Life Lifeline activity. I got to...                       \n",
      "\n",
      "  Share a loss (something that was challenging or did not go well)  \\\n",
      "0  I didn’t get opportunities from two companies ...                 \n",
      "1  Except for being sick and experiencing challen...                 \n",
      "2                                   I don’t have any                 \n",
      "3  I did not understand some assignments s well a...                 \n",
      "4                                                NaN                 \n",
      "\n",
      "  Share a blocker, if any (anything that stopped you from doing what you needed to do)  \n",
      "0  Being financially unstable has been draining m...                                    \n",
      "1  None, only temporary set backs (reception and ...                                    \n",
      "2  Data , I couldn’t join some meetings because I...                                    \n",
      "3  Spending most time in class leading to having ...                                    \n",
      "4  I forgot to login to Google classroom, until I...                                    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Focus on relevant columns\n",
    "Win = \"Share a win from the last week (what went well, something you enjoyed)\"\n",
    "Loss = \"Share a loss (something that was challenging or did not go well)\"\n",
    "Blocker = \"Share a blocker, if any (anything that stopped you from doing what you needed to do)\"\n",
    "#copy_df = df[[Win, Loss, Blocker]]\n",
    "print(copy_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "384efcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Completing my first week with Umuzi gave me co...\n",
      "1    I enjoyed introspecting myself on the basis of...\n",
      "2    Submitting all my work in time and completing ...\n",
      "3    I submitted most of the assigned assignments I...\n",
      "4    I enjoyed the Life Lifeline activity. I got to...\n",
      "Name: All_Copy, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. DATA CLEANING & PREPROCESSING\n",
    "# -----------------------------------------------------------\n",
    "# ensure we're worrking on a copy\n",
    "df = copy_df.copy()\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "df[Win] = df[Win].fillna(\"\")\n",
    "df[Loss] = df[Loss].fillna(\"\")\n",
    "df[Blocker] = df[Blocker].fillna(\"\")\n",
    "# Combine all feedback copy into one column for analysis\n",
    "df['All_Copy'] = df[Win].astype(str) + \" \" + df[Loss].astype(str) + \" \" + df[Blocker].astype(str)\n",
    "print(df['All_Copy'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fa44048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. TOKENIZATION, STOPWORD REMOVAL, LEMMATIZATION\n",
    "# -----------------------------------------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e40766e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt available\n",
      "nltk.data.path: ['C:\\\\Users\\\\Thuto/nltk_data', 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\nltk_data', 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Thuto\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print('punkt available')\n",
    "except LookupError:\n",
    "    print('punkt missing — downloading now')\n",
    "    nltk.download('punkt')  # ensure internet and correct kernel\n",
    "\n",
    "# optional: show paths NLTK searches\n",
    "import nltk.data\n",
    "print('nltk.data.path:', nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c38a6d83",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Thuto/nltk_data'\n    - 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Thuto\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m wins_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m df[Win]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocess(sentence)]\n\u001b[0;32m      3\u001b[0m losses_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m df[Loss]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocess(sentence)]\n\u001b[0;32m      4\u001b[0m blockers_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m df[Blocker]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocess(sentence)]\n",
      "Cell \u001b[1;32mIn[47], line 15\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     13\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-z\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Remove stopwords and lemmatize\u001b[39;00m\n\u001b[0;32m     17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\Users\\Thuto\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Thuto\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Thuto\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\Thuto\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\Thuto\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\Thuto\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Thuto/nltk_data'\n    - 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Thuto\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Thuto\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply preprocessing\n",
    "wins_tokens = [token for sentence in df[Win].astype(str) for token in preprocess(sentence)]\n",
    "losses_tokens = [token for sentence in df[Loss].astype(str) for token in preprocess(sentence)]\n",
    "blockers_tokens = [token for sentence in df[Blocker].astype(str) for token in preprocess(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e1d354d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wins_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 6. FREQUENCY ANALYSIS (TOP THEMES)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m wins_top5 \u001b[38;5;241m=\u001b[39m Counter(wins_tokens)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m losses_top5 \u001b[38;5;241m=\u001b[39m Counter(losses_tokens)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      6\u001b[0m blockers_top5 \u001b[38;5;241m=\u001b[39m Counter(blockers_tokens)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wins_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. FREQUENCY ANALYSIS (TOP THEMES)\n",
    "# -----------------------------------------------------------\n",
    "wins_top5 = Counter(wins_tokens).most_common(5)\n",
    "losses_top5 = Counter(losses_tokens).most_common(5)\n",
    "blockers_top5 = Counter(blockers_tokens).most_common(5)\n",
    "\n",
    "print(\"Top 5 Wins Themes:\", wins_top5)\n",
    "print(\"Top 5 Losses Themes:\", losses_top5)\n",
    "print(\"Top 5 Blockers Themes:\", blockers_top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd831db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7. SENTIMENT ANALYSIS\n",
    "# -----------------------------------------------------------\n",
    "wins_sentiment = [TextBlob(str(text)).sentiment.polarity for text in df[win_col]]\n",
    "losses_sentiment = [TextBlob(str(text)).sentiment.polarity for text in df[loss_col]]\n",
    "\n",
    "wins_summary = {\n",
    "    'positive': sum(1 for s in wins_sentiment if s > 0),\n",
    "    'neutral': sum(1 for s in wins_sentiment if s == 0),\n",
    "    'negative': sum(1 for s in wins_sentiment if s < 0)\n",
    "}\n",
    "\n",
    "losses_summary = {\n",
    "    'positive': sum(1 for s in losses_sentiment if s > 0),\n",
    "    'neutral': sum(1 for s in losses_sentiment if s == 0),\n",
    "    'negative': sum(1 for s in losses_sentiment if s < 0)\n",
    "}\n",
    "\n",
    "print(\"Wins Sentiment Summary:\", wins_summary)\n",
    "print(\"Losses Sentiment Summary:\", losses_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8. VISUALIZATIONS\n",
    "# -----------------------------------------------------------\n",
    "# Bar charts for top themes\n",
    "sns.barplot(x=[w[0] for w in wins_top5], y=[w[1] for w in wins_top5])\n",
    "plt.title(\"Top 5 Wins Themes\")\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=[l[0] for l in losses_top5], y=[l[1] for l in losses_top5])\n",
    "plt.title(\"Top 5 Losses Themes\")\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=[b[0] for b in blockers_top5], y=[b[1] for b in blockers_top5])\n",
    "plt.title(\"Top 5 Blockers Themes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word Clouds\n",
    "for name, tokens in [('Wins', wins_tokens), ('Losses', losses_tokens), ('Blockers', blockers_tokens)]:\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tokens))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{name} Word Cloud\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 9. INSIGHTS & RECOMMENDATIONS\n",
    "# -----------------------------------------------------------\n",
    "print(\\\"\\\\nRecommendations:\\\")\n",
    "print(\\\"1. Improve internet/data support for students (frequent blocker).\\\")\n",
    "print(\\\"2. Offer time management workshops (common loss theme).\\\")\n",
    "print(\\\"3. Provide financial assistance or guidance (blocker and loss theme).\\\")\n",
    "print(\\\"4. Enhance clarity in instructions and resources (loss theme).\\\")\n",
    "print(\\\"5. Continue motivational and career planning activities (win theme).\\\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 10. EXPORT RESULTS\n",
    "# -----------------------------------------------------------\n",
    "# Save summary as JSON\n",
    "import json\n",
    "summary = {\n",
    "    'wins_top5': wins_top5,\n",
    "    'losses_top5': losses_top5,\n",
    "    'blockers_top5': blockers_top5,\n",
    "    'wins_sentiment': wins_summary,\n",
    "    'losses_sentiment': losses_summary\n",
    "}\n",
    "with open('analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
